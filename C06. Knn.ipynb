{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN: $K$-Nearest Neighbors o $K$ vecinos más cercanos\n",
    "\n",
    "Es un método de clasificación no paramétrico; es decir, **no requiere asumir ninguna distribución para la variable aleatoria $X=(X_1,X_2,...,X_p)$**. Este método no requiere estimar las probabilidades desconocidas $\\pi_g$ de que un elemento seleccionado al azar provenga de la población $g$.\n",
    "\n",
    "La idea es buscar, para la nueva observación que queremos clasificar, sus $K$ vecinos más cercanos, es decir, las $K$ observaciones más cercanas respecto a una medida de distancia.\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "1) Definimos una medida de distancia adecuada para las observaciones.\n",
    "\n",
    "2) Calculamos la distancia entre la nueva observación $\\boldsymbol{x_0}$ que queremos clasificar, y las observaciones que tenemos en nuestra matriz de datos.\n",
    "\n",
    "3) Seleccionamos las $K$ observaciones más cercanas a $\\boldsymbol{x_0}$, y miramos a qué grupo pertenecen.\n",
    "\n",
    "4) Clasificamos $\\boldsymbol{x_0}$ en la población a la que pertenece una mayor proporción de sus $K$ vecinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml20.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml21.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = [12, 12]\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KNN - K vecinos más próximos\n",
    "\n",
    "Vamos a ver como vamos a usar el algoritmo KNN en scikit-learn.\n",
    "\n",
    "El algoritmo KNN se puede usar tanto en problemas de clasificación (con el estimador [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)) como en problemas de regresión (con el estimador [KNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos\n",
    "\n",
    "Para este ejemplo vamos a usar el dataset [CSM (Conventional and Social Media Movies)](https://archive.ics.uci.edu/ml/datasets/CSM+%28Conventional+and+Social+Media+Movies%29+Dataset+2014+and+2015) que contiene información de la popularidad en redes sociales de distintas películas así como las ventas en taquilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelis = pd.read_csv(\"datos_peliculas.csv\")\n",
    "pelis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a eliminar el título de las películas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelis = pelis.drop(\"pelicula\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar este dataset para probar KNN en clasificación y en regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN para problemas de clasificación\n",
    "\n",
    "Probamos KNN para clasificación, en concreto vamos a suponer que queremos predecir el género de una película en función de su popularidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "variable_objetivo_clasificacion = \"genero\"\n",
    "variables_independientes_clasificacion = pelis.drop(\n",
    "    variable_objetivo_clasificacion, axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pelis[variables_independientes_clasificacion],\n",
    "    pelis[variable_objetivo_clasificacion], test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros más importantes a la hora de usar `KNeighborsClasifier` son:\n",
    "\n",
    "- **n_neighbors**: El valor de K, es decir el número de vecinos que considerar a la hora de asignar una clase.\n",
    "- **weights**: A la hora de votar, que importancia dar a los vecinos. Si elegimos `auto` asigna la misma importancia a todos los vecinos. Si elegimos `distance` asigna importancia a los vecinos en función de la distancia de los vecinos al punto a clasificar\n",
    "- **metric**: La métrica a la hora de medir la distancia entre los puntos. Si se usa distancia de Minkowsky se puede elegir p con el parámetro `p`, que por defecto es 2 (lo que computa la distancia euclidiana)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso en particular sabemos que valor elegir de K, ya que podemos asumir que el número de categorías del dataset es el total de categorías de películas del dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_categorias = len(y_train.unique())\n",
    "k_categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador_knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                                        weights=\"uniform\")\n",
    "\n",
    "clasificador_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clasificador_knn.predict(X_test)\n",
    "f1_score(y_test, preds, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora entrenamos el estimador con el argumento de pesos `weights=\"distance\"`, vemos que funciona de forma ligeramente mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador_knn = KNeighborsClassifier(n_neighbors=10, \n",
    "                                        weights=\"distance\")\n",
    "\n",
    "clasificador_knn.fit(X_train, y_train)\n",
    "\n",
    "preds = clasificador_knn.predict(X_test)\n",
    "f1_score(y_test, preds, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el método `kneighbors` para devolver los k vecinos de un punto en concreto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distancia, indice = clasificador_knn.kneighbors(\n",
    "    [X_test.iloc[0]], n_neighbors=1)\n",
    "distancia, indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[indice[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el vecino más cercano es similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN para problemas de regresión\n",
    "\n",
    "Vamos a utilizar ahora el algoritmo KNN para un problema de regresión, KNN funciona igual para hacer regresiones, simplemente que en vez de una votación donde la clase más común entre los vecinos más próximos es la elegida, se hace una interpolación de los valores de la variable numérica objetivo de los vecinos.\n",
    "\n",
    "en concreto vamos a estimar las ventas de entradas en taquilla de una película en función de su popularidad online y presupuesto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "variable_objetivo_regresion = \"ventas\"\n",
    "variables_independientes_regresion = pelis.drop(\n",
    "    variable_objetivo_regresion, axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pelis[variables_independientes_regresion],\n",
    "    pelis[variable_objetivo_regresion], test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la implementación en sklearn `KNeighborsRegressor` para problemas de regresión. Tiene los mismos hiperparámetros que `NeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regresor_knn = KNeighborsRegressor(n_neighbors=10, weights=\"distance\")\n",
    "\n",
    "regresor_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = regresor_knn.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.abs(mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos el funcionamiento en validación cruzada tanto del clasificador como el regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_validacion_cruzada_clasificacion = np.sqrt(np.abs(\n",
    "    cross_val_score(KNeighborsClassifier(n_neighbors=k_categorias,\n",
    "                                         weights=\"distance\"), \n",
    "                X=pelis[variables_independientes_clasificacion],\n",
    "               y=pelis[variable_objetivo_clasificacion], \n",
    "               scoring=\"f1_micro\"\n",
    "        ).mean()\n",
    "      )\n",
    ")\n",
    "print(\"La puntuación F1 de KNN para clasificacion en este dataset es {:.2f}\".format(\n",
    "    error_validacion_cruzada_clasificacion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
