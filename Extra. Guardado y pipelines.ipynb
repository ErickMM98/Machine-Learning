{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"datos_pipelines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en un apartado anterior, en este ejemplo vamos a modificar cada variable en función de su tipo. Al conjunto de pasos que siguen los datos se le llama comúnmente **Pipelines** (literalmente, sistemas de tuberias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml19.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inicio**\n",
    "\n",
    "vamos a modificar dos transformadores de scikitlearn para que sean compatibles con pipelines. Este paso es necesario en la version actual de scikit-learn, pero seguramente será arreglado en el futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Modificar transformadores\n",
    "\n",
    "class BinarizadorCategorico(preprocessing.LabelBinarizer):\n",
    "    def fit(self, X, y=None):\n",
    "        super(BinarizadorCategorico, self).fit(X)\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return super(BinarizadorCategorico, self).transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(BinarizadorCategorico, self).fit(X).transform(X)\n",
    "    \n",
    "    \n",
    "class CodificadorCategorico(preprocessing.LabelEncoder):\n",
    "    def fit(self, X, y=None):\n",
    "        super(CodificadorCategorico, self).fit(X)\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return super(CodificadorCategorico, self).transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(CodificadorCategorico, self).fit(X).transform(X)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a definir los transformadores de forma similar a como hicimos la última vez, solo que en vez de usar `OneHotEncoder` vamos a usar nuestra version de sklearn `LabelBinarizer` que hace la codificación one hot directamente sobre una variable categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar nombres de las columnas\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "col_numericas =  ['col_inexistente1', 'col2', 'col3', 'col_outliers', 'col_outliers2']\n",
    "col_categorica = ['col_categorica']\n",
    "col_texto = ['col_texto']\n",
    "col_ordinal = ['col_ordinal']\n",
    "\n",
    "imputador = SimpleImputer(missing_values=np.nan, copy=False)\n",
    "escalador = preprocessing.StandardScaler()\n",
    "\n",
    "transformador_ordinal = CodificadorCategorico()\n",
    "transformador_categorico = BinarizadorCategorico()\n",
    "\n",
    "transformador_texto = feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "estimador = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con el Binarizador transformamos como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[col_categorica]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformador_categorico.fit_transform(datos[col_categorica])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que es mucho más sencillo que cómo lo hicimos la vez anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.OneHotEncoder().fit_transform(\n",
    "    transformador_ordinal.fit_transform(datos[col_categorica]).reshape(1000,1)\n",
    ").toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un pipeline de sklearn se define como una secuencia de pasos. Cada paso se define con una tupla de forma `(nombre del paso, transformador)`\n",
    "\n",
    "Por ejemplo, si queremos crear un pipeline que procese las variables numéricas, primero imputándolas y después estandarizandolas, podriamos crear un pipeline como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "transformador_numerico = Pipeline(\n",
    "     [('imputador', imputador), ('escalador', escalador)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformador_numerico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos definidos los pasos que queremos aplicar a cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformador_numerico.fit_transform(datos[col_numericas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero seguimos teniendo el mismo problema de siempre, como podemos aplicar determinados transformadores a determinadas variables?\n",
    "\n",
    "Bien, para los casos en los que tenemos un dataframe de Pandas, una opcion es crear un transformador customizado que simplemente selecciones columnas de un dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `scikit-learn` podemos crear nuestros propios transformadores creando una clase que herede de `TransformerMixin` y que tenga el mètodo `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class TransformadorBase(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear dos transformadores, un `DenseTransformer` que convierte una matriz `sparse` en un array (tomado de [mlxtend](http://rasbt.github.io/mlxtend/), y `ColumnExtractor` que devuelve una selección de columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "# http://rasbt.github.io/mlxtend/\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X=X, y=y)\n",
    "\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.columns].to_numpy()\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo si creamos un ColumnExtractor pasandole las columnas numéricas tenemos un transformador que podemos usar en un pipeline y que simplemente selecciona un subgrupo de columnas (devolviendolas como matriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cext = ColumnExtractor(columns=col_numericas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cext.fit_transform(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora los pipelines para cada tipo de variable. En algunos casos he añadido pasos adicionales por dos motivos. El primero, que determinados elementos de sklearn esperan datos ligeramente distintos. En segundo lugar, para conseguir que la salida de cada pipeline tenga la misma forma (un array de arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "\n",
    "pipeline_numerico = Pipeline([\n",
    "    ['selector_numerico', ColumnExtractor(columns=col_numericas)],\n",
    "    ['transformador_numerico', transformador_numerico]\n",
    "])\n",
    "\n",
    "pipeline_numerico.fit_transform(datos)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "\n",
    "def mi_funcion1(x):\n",
    "    return x[:,0]\n",
    "\n",
    "pipeline_texto = Pipeline([\n",
    "        ['selector_texto', ColumnExtractor(columns=col_texto)],\n",
    "        ['transformador_dim', preprocessing.FunctionTransformer(mi_funcion1, validate=False)],\n",
    "        ['transformador_texto', transformador_texto],\n",
    "        ['texto_array', DenseTransformer()]\n",
    "    ])\n",
    "\n",
    "pipeline_texto.fit_transform(datos)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "\n",
    "pipeline_categorico = Pipeline([\n",
    "    ['selector_categorica', ColumnExtractor(columns=col_categorica)],\n",
    "    ['transformador_categorico', transformador_categorico]\n",
    "])\n",
    "\n",
    "pipeline_categorico.fit_transform(datos)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del pipeline ordinal hay que manipular las dimensiones de los arrays dado que trabaja con un vector de dimension 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########\n",
    "\n",
    "def mi_funcion2(x):\n",
    "    return np.vstack(x[:])\n",
    "\n",
    "pipeline_ordinal = Pipeline([\n",
    "    ['selector_ordinal', ColumnExtractor(columns=col_ordinal)],\n",
    "    ['transformador_dim1', preprocessing.FunctionTransformer(mi_funcion1, validate=False)],\n",
    "    ['transformador_ordinal', transformador_ordinal],\n",
    "    ['transformador_dim2', preprocessing.FunctionTransformer(mi_funcion2, validate=False)],\n",
    "])\n",
    "\n",
    "\n",
    "pipeline_ordinal.fit_transform(datos)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos una manera de, dado un conjunto de datos, separarlos y aplicar distintas transformaciones a cada variable. Nos falta una manera de, una vez se han transformado, reunirlas de nuevo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello podemos usar `FeatureUnion`, que simplemente toma un conjunto de pasos de un pipeline y los une."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "\n",
    "pipeline_procesado = FeatureUnion([\n",
    "    ('variables_numericas', pipeline_numerico),\n",
    "    ('variables_ordinales', pipeline_ordinal),\n",
    "    ('variables_texto', pipeline_texto),\n",
    "    ('variables_categoricas', pipeline_categorico),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_procesado.fit_transform(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, necesitamos añadir un estimador al final para predecir con base a los datos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "\n",
    "pipeline_estimador = Pipeline([\n",
    "    ('procesador', pipeline_procesado),\n",
    "    ('estimador', estimador)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "pipeline_estimador.fit(datos,datos[\"objetivo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_estimador.predict(datos)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"objetivo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El beneficio de los pipelines, no solo es tener codigo mas legible y poder gestionar de forma ordenada todo el ciclo de vida del modelado, sino que los pipelines tienen todos los beneficios de los objetos de scikitlearn, por ejemplo, podemos usar validacion cruzada directamente con el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(pipeline_estimador, X=datos.drop('objetivo', axis=1), y=datos[\"objetivo\"], scoring='roc_auc', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_estimador.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"modelo_final.pickle\", \"wb\") as file:\n",
    "    pickle.dump(pipeline_estimador, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('modelo_final.pickle', \"rb\") as file:\n",
    "    mi_modelo = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_modelo.predict(datos[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
