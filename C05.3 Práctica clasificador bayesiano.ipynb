{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los clasificadores Naive Bayes (Naive Bayes Classifier - NBC) se usan , como su nombre indica, para problemas de clasificación, y en concreto se pueden aplicar para texto. \n",
    "\n",
    "En este ejemplo vamos a implementar un modelo NBC a un dataset de un portal de Noticias muy famoso en España. Cada usuario comparte un link a una noticia y le puede asignar una categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "noticias = pd.read_csv(\"noticias.csv\").iloc[:2000,]\n",
    "\n",
    "noticias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable objetivo es `categoria` y la variable independiente es `descripcion` que contiene la descripcion de la noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias.categoria.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay noticias de 3 tipos de categorías distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los clasificadores Naive Bayes esperan como input un vector, así que para poder entrenarlos tenemos que vectorizar el texto. Para ello una buena opción es usar vectorización Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a modificar nuestro vectorizador añadiendole dos parámetros:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Eliminar Stopwords**\n",
    "\n",
    "Stopwords (palabras vacías) son palabras que no tienen ningún contenido semántico. Por ejemplo, en la frase `el perro ladra` el artículo `el` no aporta ningún valor a la frase.\n",
    "\n",
    "\n",
    "Me he descargado una lista de stopwords de castellano de este [repositorio en Github](https://github.com/stopwords-iso/stopwords-es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"stopwords-es.json\") as fname:\n",
    "    stopwords_es = json.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_es[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Eliminar acentos**\n",
    "\n",
    "También vamos a eliminar los acentos de las palabras, ésto tiene una ventaja, y es que si en el conjunto de datos no tenemos confianza en la calidad de los escritores, al eliminar los acentos evitamos que si un escritor no usa acentos no considere sus palabras como palabras distintas.\n",
    "\n",
    "En castellano, esto tiene un problema, y es que hay palabras con significado distinto que sólo se diferencian por la existencia de una tilde (se llaman palabras con [acento diacrítico](https://es.wikipedia.org/wiki/Acento_diacr%C3%ADtico) (por ejemplo, `de y dé`). Asumimos pues que el impacto de estas palabras no es muy alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador = TfidfVectorizer(strip_accents=\"unicode\", stop_words=stopwords_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador.fit_transform(noticias.descripcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicha matriz nos indica que tenemos 16495 artículos que tienen 59969 palabras distintas (sin contar acentos o stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que los vectorizadores devuelven una matriz sparse (escasa) creamos un transformador que las convierta a densas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "# http://rasbt.github.io/mlxtend/\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn tiene tres implementaciones del clasificador [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html), `GaussianNB, BernoulliNB y MultinomialNB`, y cada una se diferencia por como calcula las probabilidades de que cada elemento aparezca en los datos.\n",
    "\n",
    "[GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) asume que los datos siguen una distribución Gausiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gaussiano = make_pipeline(\n",
    "    vectorizador,\n",
    "    DenseTransformer(),\n",
    "    GaussianNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gaussiano.fit(X=noticias.descripcion, y=noticias.categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gaussiano.predict(noticias.descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline_gaussiano, noticias.descripcion, noticias.categoria, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la validación cruzada con la puntuación F1 nos da un error `ValueError: Target is multiclass but average='binary'. Please choose another average setting.`, esto es por que las medidas de clasificación tienen distintas maneras de calcularse en funcion de si es un caso de clasificación binaria (el parámetro por defecto) o clasificación multiclase. \n",
    "\n",
    "En concreto para el caso de la puntuación F1, nos permite los siguientes tipos de cálculos.\n",
    "\n",
    "- **binary**: Devuelve la puntuación para la clase especificada en el argumento `pos_label`. Solo se puede aplicar en clasificación binaria. Este es el caso que vimos en el apartado de medidas de evaluación de modelos de clasificación.\n",
    "\n",
    "- **micro**: Cuenta todos el número total de Verdaderos positivos (TP), Falsos Negativos (FN) y Falsos Positivos (FP) y calcula una precisión y sensibilidad total y obtiene el F1. Es mejor cuando hay clases no balanceadas (muchos casos más de una clase que de las demás).\n",
    "\n",
    "- **macro**: Calcula la precisión y sensibilidad media de cada clase, hace su media aritmética y calcula el parámetro F1.\n",
    "\n",
    "- **weighted**: Calcula la precisión y sensibilidad media de cada clase, hace su media ponderada por el número de observaciones de cada clase y calcula el parámetro F1.\n",
    "\n",
    "- **samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def f1_multietiqueta(estimador, X, y):\n",
    "    preds = estimador.predict(X)\n",
    "    return f1_score(y, preds, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline_gaussiano, noticias.descripcion, noticias.categoria, scoring=f1_multietiqueta,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto antes, el vectorizador TF-IDF coge por defecto todas las palabras (removiendo palabras sin aporte de información). Podemos restringir la cantidad de palabras que considera con el parámetro `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gaussiano = make_pipeline(\n",
    "    TfidfVectorizer(strip_accents=\"unicode\", stop_words=stopwords_es, max_features=1000),\n",
    "    DenseTransformer(),\n",
    "    GaussianNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline_gaussiano, noticias.descripcion, noticias.categoria, scoring=f1_multietiqueta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dos implementaciones de clasificadores NB más utilizadas para clasificación de texto son [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) (que asume que la distribución de probabilidades de las palabras en el conjunto de datos sigue una distribución multinomial y [BernouilliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB) , que asume que siguen una distribución de Bernouilli multivariable (donde la existencia de cada palabra se considera que es una variable binaria distinta).\n",
    "\n",
    "Según la documentación, el clasificador MultinomialNB funciona bien con vectores TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_multinomial = make_pipeline(\n",
    "    TfidfVectorizer(strip_accents=\"unicode\", stop_words=stopwords_es, max_features=500),\n",
    "    DenseTransformer(),\n",
    "    MultinomialNB(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline_multinomial, noticias.descripcion, noticias.categoria,\n",
    "                scoring=f1_multietiqueta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que efectivamente, `MultinomialNB` parece funcionar mejor que `GaussianNB` para vectores TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el clasificador [BernouilliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB) se necesita tener los vectores de palabras cono vectores binarios (1 si la palabra existe o 0 si no), así que en vez de usar TfidfVectorizer en este caso usaremos el CountVectorizer pasandole el parámetro `binary=True` para que devuelva 1 ó 0 en vez del número de veces que aparece la palabra en la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizador_count = CountVectorizer(stop_words=stopwords_es, binary=True, \n",
    "                                     strip_accents=\"unicode\", max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una funcionalidad interesante del CountVectorizer es que nos permite ver cuantas veces aparece cada palabra en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador_count.fit(noticias.descripcion)\n",
    "vectorizador_count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_bernouilli = make_pipeline(\n",
    "    vectorizador_count,\n",
    "    DenseTransformer(),\n",
    "    BernoulliNB(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipeline_bernouilli, noticias.descripcion, noticias.categoria, scoring=f1_multietiqueta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
