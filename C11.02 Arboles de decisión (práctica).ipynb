{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el dataset del [Titanic](https://www.kaggle.com/c/titanic/data), que contiene los datos de los pasajeros del Titanic, famoso barco que se hundió en su primer viaje en 1912. Dicho dataset contiene los datos de los pasajeros así como información sobre si sobrevivieron o no.\n",
    "\n",
    "**Descripción de los datos**\n",
    "\n",
    "* superviviente \t El pasajero sobrevivió \t0 = No, 1 = Si\n",
    "\n",
    "* clase_billete \t Clase de camarote \t1 = Primera clase, 2 = Segunda, 3 = Tercera clase (la más pobre)\n",
    "\n",
    "* genero \t         hombre/mujer \t\n",
    "\n",
    "* edad \t\n",
    "\n",
    "* n_hermanos_esposos Número de hermanos o pareja a bordo del Titanic\n",
    "\n",
    "* n_hijos_padres     Número de hijos o padres a bordo del Titanic \n",
    "\n",
    "* precio_billete \t\n",
    "\n",
    "* puerto_salida      Puerto donde el pasajero tomó el barco (C=Cherbourg, Q=Queenstown, S=Southampton)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de creación de árboles están en el submódulo de `sklearn.tree` \n",
    "\n",
    "En cuanto al tipo de algoritmo para crear árboles, scikit-learn [usa una versión optimizada del algoritmo CART](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) *(Classification and Regression Trees)*, que  permite usar árboles de decisión tanto para problemas de clasificación como de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_categoricas = [\"genero\", \"puerto_salida\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_categoricos = pd.get_dummies(datos[columnas_categoricas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasajeros = (\n",
    "    pd.concat([\n",
    "        datos.drop(columnas_categoricas, axis=1),\n",
    "        datos_categoricos\n",
    "    ],axis=1\n",
    "    )\n",
    ")\n",
    "pasajeros.edad = pasajeros.edad.fillna(pasajeros.edad.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasajeros.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.fit(pasajeros.drop(\"superviviente\", axis=1), pasajeros.superviviente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(arbol, pasajeros.drop(\"superviviente\", axis=1), pasajeros.superviviente, scoring=\"roc_auc\", \n",
    "                cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una funcionalidad interesante que tienen los arboles de decision en `sklearn` es que se pueden visualizar con `Graphviz`. Para ello tenemos que instalar la libreria con `pip install graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede exportar el árbol y abrirlo posteriormente con graphviz desde la terminal (o desde la página http://webgraphviz.com/ que renderiza archivos de graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "tree.export_graphviz(arbol, out_file=\"arbol.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra funcionalidad que tienen los árboles de decisión en sklearn es que nos dan una indicación de la importancia de cada variable en el modelo, almacenada en el atributo `feature_importances_`. Calcula la importancia en función de la ganancia de información de cada variable, es decir, que variables separan mejor las distintas clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(\n",
    "    pasajeros.drop(\"superviviente\", axis=1),\n",
    "    arbol.feature_importances_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, en este árbol vemos que las variables que tienen más peso a la hora de decidir si un pasajero sobrevive o no son la edad, el género y el precio del billete (¡las mujeres y los niños primero!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Éstos son los parámetros más importantes para los modelos DecisionTreeClassifier de sklearn:\n",
    "\n",
    "* criterion : El criterio para calcular la reducción de impureza (ganancia de información) al hacer una partición. Se puede elegir entre `gini`, o `entropy` \n",
    "\n",
    "* max_depth : La profundidad máxima del árbol. Definimos profundidad como el número de nodos que atraviesa una observación *(cuantas \"preguntas\" se le hacen)*.\n",
    "\n",
    "* max_features:  El máximo numero de particiones potenciales se consideran al evaluar un nodo.\n",
    "\n",
    "* max_leaf_nodes : Límite de hojas para el árbol.\n",
    "\n",
    "* min_impurity_decrease : la ganancia de información mínima en un nodo para hacer una partición. (Si no hay ninguna partición que cumpla este criterio, se para el desarrollo del árbol en dicho nodo).\n",
    "\n",
    "* class_weight : Para clases imbalanceadas, podemos pasar el argumento class_weight, como un diccionario de `{clase: peso}` para que sklearn tenga en cuenta los pesos. Alternativamente, podemos pasar el string `balanced` para que sklearn genere pesos en función del número de muestras de cada clase.\n",
    "\n",
    "Los árboles de decisión tienden a sobreajustar cuando tienen demasiada complejidad. Los siguientes parámetros ayudan a controlarla:\n",
    "\n",
    "* min_samples_leaf : Mínimo número de observaciones en un nodo para considerar un nodo como una hoja. Por defecto es 1, ésto significa que sklearn solo para de hacer particiones cuando un nodo tiene sólo 1 observación.\n",
    "\n",
    "* min_samples_split : Mínimo número de observaciones en un nodo para generar una partición. Por defecto es 2, esto significa que sklearn por defecto siempre va a intentar particionar un nodo con 2+ elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol_simple = tree.DecisionTreeClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol_simple.fit(pasajeros.drop(\"superviviente\", axis=1), pasajeros.superviviente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(arbol_simple, out_file=\"arbol_simple.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(arbol_simple, pasajeros.drop(\"superviviente\", axis=1), \n",
    "                pasajeros.superviviente, scoring=\"roc_auc\", cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol_balanceado = tree.DecisionTreeClassifier(max_depth=3, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol_balanceado.fit(pasajeros.drop(\"superviviente\", axis=1), pasajeros.superviviente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(arbol_balanceado, out_file=\"arbol_balanceado.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(arbol_balanceado, pasajeros.drop(\"superviviente\", axis=1), \n",
    "                pasajeros.superviviente, scoring=\"roc_auc\", cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(arbol_balanceado, pasajeros.drop(\"superviviente\", axis=1), pasajeros.superviviente,\n",
    "                scoring=\"precision\", cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además del algoritmo CART para generar árboles, scikit-learn también proporciona una clase de arboles llamada ExtraTreeClassifier, o Extremely Random Trees (Árboles Extremadamente Aleatorios). En estos árboles, en lugar de seleccionar en cada nodo la párticion que proporciona la mayor ganancia de información, ¡se decide una partición al azar!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol_aleatorio = tree.ExtraTreeClassifier(max_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol_aleatorio.fit(pasajeros.drop(\"superviviente\", axis=1), pasajeros.superviviente)\n",
    "\n",
    "tree.export_graphviz(arbol_aleatorio, out_file=\"arbol_aleatorio.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles aleatorios funcionan peor que los árboles de decisión por sí mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(arbol_aleatorio, pasajeros.drop(\"superviviente\", axis=1), pasajeros.superviviente, scoring=\"roc_auc\",\n",
    "                cv=10).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
